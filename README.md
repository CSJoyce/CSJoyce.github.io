# Data Science Portfolio

## Contents
- ## R
  - **[Home Credit Default Risk](https://github.com/CSJoyce/HomeCreditProject/blob/main)**
    - Objective Summary: HomeCredit Inc. aims to identify high-risk customers who are likely to default on their loans, as well as identify low-risk customers who have little to no line of credit.  The objective of this project is to construct predictive models using historical data to distinguish customers based on their ability to repay their loan.
    - Solution: Utilizing an XGBoost model, expanded HomeCredit's customer base to customers that were predicted to not default on loans, even with a sparse credit history.  Additionally, used a random forest model to identify customers who were at high risk for defaulting to reduce business costs.
    - Personal Contribution: As part of the data modeling process, I constructed a Naive Bayes classifier and LightGBM model in attempts to improve classsification performance relative to a majority-based classifier for the dataset.
    - Value: From our models, we were able to improve both the performance of the classification of low-risk, non-defaulting customers and the identification of high-risk, defaulting customers relative to the performance of majority-based classification of the data.  This allows HomeCredit Inc. to both extend loans to customers who are confidently low-risk and avoid incurring costs from defaulting by customers who are at higher likelihood to do so.
    - Reflection: We encountered the most difficulty in the process of exploratory data analysis.  As this was a large dataset consisting of hundreds of thousands of rows and over a hundred features, there was a large amount of missing data.  We experimented with many different techniques in the data cleaning process.  Ultimately, we decided to reduce complexity by eliminating features that contributed nothing to the predictive ability of our models, and impute data to replace missing values.  I found that training individual models took much more time than expected, and therefore the tuning and model-specific preprocessing that is done beforehand is important to evaluate before training models for hours on end to reach mediocre results or a non-functioning model.

  - **[Swire Coca-Cola Customer Relationship Management](https://github.com/CSJoyce/Swire-Project/blob/main)**
    - Objective Summary:  Swire Coca-Cola faces an issue with identifying customers in their book of business to offload to White Truck services within their US market. Businesses/customers currently on Red Truck services take up a lot of resources.  Additionally, there is a need to segment and profile customers who purchase below 400 gallons per year to guide strategic outreach. Thus, we aim to identify who and who not to offload onto the White Truck services.
      - The primary objective of this project was to build a predictive and descriptive modeling framework that:
        - Identified customer segments with consistently low ordering volumes.
        - Predicted whether a customer is likely to purchase fewer than 400 gallons in Year 2.
        - Explored operational and demographic characteristics (e.g., LMP status, CO2 status, delivery channels) to uncover behavioral patterns.
        - Supported targeted engagement strategies by clustering customers based on ordering behavior and profile.
    - Solution:  Various solutions were reached in the approach to this problem.  With an emphasis on cost savings and potential revenue, we suggested new yearly order thresholds for offloading customers to 3rd party distribution services.  Cost savings figures were estimated to be ~$10.3m, with revenue retention figures totaling over $200m. Utilizing supervised learning techniques, we were able to confidently predict likelihood of local market partnership for new customers, which was a key market for S.C.C.U..  LMP membership was found to be a strong indicator for high order growth.  Additionally,  analysis was done on the existing order threshold for S.C.C.U.. Using both k-prototype and density based clustering, we make note of attributes for customers who currently order below the 400 gallon per year threshold but have high growth potential, as well as customers with declining month-by-month orders.  Clusters were also used in geographical analysis of customer locations to suggest logistical plans for high and low potential customers.
    - Personal Contribution: Prior to modeling, I performed standard exploratory data analysis to provide exploratory insights into the historical data pertaining to Swire Coca Cola.  This consisted of various preparation steps and transformations intended to provide usable data for analytical usage, and discover patterns or attributes of interest to aid in solving the presented business problem.  For the data modeling process, I performed Principle Component Analysis, ARIMA-based forecasting, and various clustering methods.  The reasoning behind PCA was to identify customer attributes that strongly contribute to their respective order amounts, in cases and gallons. ARIMA forecasting was then performed on specific customer groups and Principle Components to observe seasonal ordering patterns and predictions for future orders placed by these customers and those with highly important features observed in PCA.  Clustering was performed in order to find interesting relationships between customers and their ordering habits, for customers who order below and above the existing yearly cutoff threshold.  Specifically, I aimed to seek out common characteristics for customers who tend to have similar order patterns, and apply these methods based on monthly and yearly order totals. I continued to emphasize the two customer groups of interest, gallon-only Local Market Partners, and all customers, in these unsupervised models.
    - Business Value: In this project, we were able to estimate overall cost savings from offloading customers without growth potential, and capture possibly forfeited revenue from those who show potential growth who would otherwise be offloaded to white truck services.  We provided suggestions for new metrics that should be utilized to maximize revenue from customers that can confidently be kept on red truck logistic services.  Additionally, we discovered insights for shared attributes among high and low potential customers to provide Swire with profiles to compare newly signed customers with these groups, and suggested logistic plans for non-qualifying customers based on their proximity to "hubs" of existing customers who can be easily served with red truck distribution.
    - Reflection: As a group, we encountered issues with preliminary compilation of our findings into a concise storyboard.  It seemed that while our individual conclusions provided valuable potential solutions to the problem, they did not mesh well together to provide an overall case.  With this in mind, we discussed a main theme to base our findings off of that both solved the business problem and did not require major revisions to our individual efforts.  Personally, I learned that it is unwise to "zero in" on specific modeling methods and try to force them to work in alignment with the overall objective.  In other words, I was working for the models rather than have the models work for me!  With this in mind, I should have considered spreading my efforts across various methods, and focus only on those that could provide real business value while staying within the boundaries of the group objective.
   

- ## Python
  - **[Financial Market Pipeline](https://github.com/CSJoyce/FinPipeline/blob/main)**
    - Objective Summary: This data engineering project creates cumulative daily reports of stock performance statistics through a combination of near real-time data with historical data.  The end goal for this project is to create serviceable data in an OLAP format to be used by market analysts and financial data scientists.
    - Approach: Using an API to fetch relative strength index for a range of popular stock tickers, I utilized pandas and SQLite operations to combine this data with historical year data through CSVs obtained with Kaggle.  After aggregation, this data was ultimately stored in a Google Cloud bucket to be serviceable by a market analyst team to identify stock trends over time based on the combined price data.
  - **[CalEnviroScreen Bay Area Pollution Analysis](https://github.com/CSJoyce/CESBA/blob/main)**
    - Objective Summary: Depending on their proximity to industrial zones and waste processing facilities, populations of Bay Area counties may be disproportionately affected in terms of likelihood of disease as well as some socioeconomic factors. This project discusses the data findings obtained from the California Office of Environmental Health Hazard Assessment in terms of these parameters.
    - Findings: It was observed that medical condition categories such as low birth weight and percentage of populations with asthma had noticeable correlation with the underlying pollution factors with relevance to those conditions. It was interesting to see the averages of different pollutant levels in Bay Area counties and how some counties had much higher prevalence of certain pollutants, which contributed to higher amounts of disease compared to other counties. When compared to the rest of the state of California, the Bay Area counties had lower levels of pollution burden, populations with lack of education, populations living in poverty, and populations who are unemployed. I believe these higher levels are a result of the burdens set on these counties by pollution factors and hazardous waste, which leads to more reports of disease for these areas.
   


  

